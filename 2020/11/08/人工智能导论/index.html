<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>人工智能导论 | Komorebi</title><meta name="description" content="机器学习中训练集、验证集和测试集的作用   训练集 作用是用来拟合模型，通过设置分类器的参数，训练分类模型。后续结合验证集作用时，会选出同一参数的不同取值，拟合出多个分类器。   验证集 作用是当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。如svn中的参数c和核函数等。   测试集"><meta name="keywords" content="python,ai算法"><meta name="author" content="komorebi"><meta name="copyright" content="komorebi"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://feeblebird.github.io/2020/11/08/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="人工智能导论"><meta property="og:url" content="http://feeblebird.github.io/2020/11/08/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA/"><meta property="og:site_name" content="Komorebi"><meta property="og:description" content="机器学习中训练集、验证集和测试集的作用   训练集 作用是用来拟合模型，通过设置分类器的参数，训练分类模型。后续结合验证集作用时，会选出同一参数的不同取值，拟合出多个分类器。   验证集 作用是当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。如svn中的参数c和核函数等。   测试集"><meta property="og:image" content="https://i.loli.net/2020/08/05/GRYrANb2noBKXMh.jpg"><meta property="article:published_time" content="2020-11-08T07:50:45.000Z"><meta property="article:modified_time" content="2020-11-23T04:10:02.655Z"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"limitCount":50,"languages":{"author":"作者: komorebi","link":"链接: ","source":"来源: Komorebi","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-11-23 12:10:02'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 5.0.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="https://i.loli.net/2020/08/02/WuXQnAJZ6twj5HU.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">32</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">35</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text"> 机器学习中训练集、验证集和测试集的作用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#knn%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text"> KNN算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#knn%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8A%BF%E5%92%8C%E5%8A%A3%E5%8A%BF"><span class="toc-number">2.1.</span> <span class="toc-text"> KNN算法的优势和劣势</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#knn%E7%AE%97%E6%B3%95%E4%BC%98%E7%82%B9"><span class="toc-number">2.1.0.1.</span> <span class="toc-text"> KNN算法优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#knn%E7%AE%97%E6%B3%95%E7%BC%BA%E7%82%B9"><span class="toc-number">2.1.0.2.</span> <span class="toc-text"> KNN算法缺点</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#knn%E5%8F%82%E6%95%B0%E6%A6%82%E8%BF%B0"><span class="toc-number">3.</span> <span class="toc-text"> KNN参数概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-knn%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B"><span class="toc-number">4.</span> <span class="toc-text"> 二. KNN代码实例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#knn%E5%92%8Ckmeans"><span class="toc-number">4.1.</span> <span class="toc-text"> KNN和Kmeans</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8A%E8%AF%BEknn%E4%BB%A3%E7%A0%81"><span class="toc-number">5.</span> <span class="toc-text"> 上课knn代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#note"><span class="toc-number">5.1.</span> <span class="toc-text"> Note:</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E"><span class="toc-number">5.1.0.0.1.</span> <span class="toc-text"> 返回</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8A%E8%AF%BEknn%E4%BB%A3%E7%A0%81-2"><span class="toc-number">6.</span> <span class="toc-text"> 上课knn代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">7.</span> <span class="toc-text"> 朴素贝叶斯</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">8.</span> <span class="toc-text"> 决策树</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kmeans"><span class="toc-number">9.</span> <span class="toc-text"> kmeans</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#randomforest"><span class="toc-number">10.</span> <span class="toc-text"> randomForest</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E6%A6%82%E5%BF%B5"><span class="toc-number">11.</span> <span class="toc-text"> 深度学习常用数学概念</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/08/05/GRYrANb2noBKXMh.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Komorebi</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">人工智能导论</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-11-08T07:50:45.000Z" title="发表于 2020-11-08 15:50:45">2020-11-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-11-23T04:10:02.655Z" title="更新于 2020-11-23 12:10:02">2020-11-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/">python</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="机器学习中训练集-验证集和测试集的作用"><a class="markdownIt-Anchor" href="#机器学习中训练集-验证集和测试集的作用"></a> 机器学习中训练集、验证集和测试集的作用</h1>
<ul>
<li>
<p>训练集</p>
<p>作用是用来拟合模型，通过设置分类器的参数，训练分类模型。后续结合验证集作用时，会选出同一参数的不同取值，拟合出多个分类器。</p>
</li>
<li>
<p>验证集</p>
<p>作用是当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。如svn中的参数c和核函数等。</p>
</li>
<li>
<p>测试集</p>
<p>通过训练集和验证集得出最优模型后，使用测试集进行模型预测。用来衡量该最优模型的性能和分类能力。即可以把测试集当做从来不存在的数据集，当已经确定模型参数后，使用测试集进行模型性能评价。</p>
</li>
<li>
<p>对原始数据进行三个数据集的划分，也是为了防止模型过拟合。当使用了所有的原始数据去训练模型，得到的结果很可能是该模型最大程度地拟合了原始数据，亦即该模型是为了拟合所有原始数据而存在。当新的样本出现，再使用该模型进行预测，效果可能还不如只使用一部分数据训练的模型。</p>
</li>
</ul>
<h1 id="knn算法"><a class="markdownIt-Anchor" href="#knn算法"></a> KNN算法</h1>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/listenfwind/p/10311496.html">搬运</a></p>
<ul>
<li>
<p>算法概述</p>
<p>KNN可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一，注意KNN算法是有监督学习中的分类算法，它看起来和另一个机器学习算法Kmeans有点像（Kmeans是无监督学习算法），但却是有本质区别的。那么什么是KNN算法呢，接下来我们就来介绍介绍吧。</p>
</li>
<li>
<p>knn算法介绍</p>
<p>KNN的全称是K Nearest Neighbors，意思是K个最近的邻居，从这个名字我们就能看出一些KNN算法的蛛丝马迹了。K个最近邻居，毫无疑问，K的取值肯定是至关重要的。那么最近的邻居又是怎么回事呢？<strong>其实啊，KNN的原理就是当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别</strong>。听起来有点绕，还是看看图吧。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/VMEqCTOlgxnwGWp.png" alt="image.png" /></p>
<p>图中绿色的点就是我们要预测的那个点，假设K=3。那么KNN算法就会找到与它距离最近的三个点（这里用圆圈把它圈起来了），看看哪种类别多一些，比如这个例子中是蓝色三角形多一些，新来的绿色点就归类到蓝三角了。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/PU29brsAnMW8XD1.png" alt="image.png" /></p>
<p>但是，<strong>当K=5的时候，判定就变成不一样了</strong>。这次变成红圆多一些，所以新来的绿点被归类成红圆。从这个例子中，我们就能看得出K的取值是很重要的。</p>
<p>明白了大概原理后，我们就来说一说细节的东西吧，主要有两个，<strong>K值的选取</strong>和<strong>点距离的计算</strong>。</p>
</li>
<li>
<p>距离计算</p>
<p>要度量空间中点距离的话，有好几种度量方式，比如常见的曼哈顿距离计算，欧式距离计算等等。不过通常KNN算法中使用的是欧式距离，这里只是简单说一下，拿二维平面为例，，二维空间两个点的欧式距离计算公式如下：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/M8rAT6NulLpJImD.png" alt="image.png" /></p>
<p>这个高中应该就有接触到的了，其实就是计算（x1,y1）和（x2,y2）的距离。拓展到多维空间，则公式变成这样：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/5W4dwAbvnY9iyVh.png" alt="image.png" /></p>
<p>这样我们就明白了如何计算距离，KNN算法最简单粗暴的就是将预测点与所有点距离进行计算，然后保存并排序，选出前面K个值看看哪些类别比较多。但其实也可以通过一些数据结构来辅助，比如最大堆，这里就不多做介绍，有兴趣可以百度最大堆相关数据结构的知识。</p>
</li>
<li>
<p>k值选择</p>
<p>通过上面那张图我们知道K的取值比较重要，那么该如何确定K取多少值好呢？答案是通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。</p>
<p>通过交叉验证计算方差后你大致会得到下面这样的图：<br />
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/VnhN8vqzP4rjBHU.png" alt="image.png" /></p>
<p>这个图其实很好理解，当你增大k的时候，一般错误率会先降低，因为有周围更多的样本可以借鉴了，分类效果会变好。但注意，和K-means不一样，当K值更大的时候，错误率会更高。这也很好理解，比如说你一共就35个样本，当你K增大到30的时候，KNN基本上就没意义了。</p>
<p>所以选择K点的时候可以选择一个较大的临界K点，当它继续增大或减小的时候，错误率都会上升，比如图中的K=10。具体如何得出K最佳值的代码，下一节的代码实例中会介绍。</p>
</li>
<li>
<p>knn特点</p>
<p>KNN是一种<strong>非参的</strong>，<strong>惰性</strong>的算法模型。什么是非参，什么是惰性呢？</p>
<p><strong>非参</strong>的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。</p>
<p><strong>惰性</strong>又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。</p>
<h2 id="knn算法的优势和劣势"><a class="markdownIt-Anchor" href="#knn算法的优势和劣势"></a> KNN算法的优势和劣势</h2>
<p>了解KNN算法的优势和劣势，可以帮助我们在选择学习算法的时候做出更加明智的决定。那我们就来看看KNN算法都有哪些优势以及其缺陷所在！</p>
<h4 id="knn算法优点"><a class="markdownIt-Anchor" href="#knn算法优点"></a> KNN算法优点</h4>
<ol>
<li>简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。</li>
<li>模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。</li>
<li>预测效果好。</li>
<li>对异常值不敏感</li>
</ol>
<h4 id="knn算法缺点"><a class="markdownIt-Anchor" href="#knn算法缺点"></a> KNN算法缺点</h4>
<ol>
<li>对内存要求较高，因为该算法存储了所有训练数据</li>
<li>预测阶段可能很慢</li>
<li>对不相关的功能和数据规模敏感</li>
</ol>
<p>至于什么时候应该选择使用KNN算法，sklearn的这张图给了我们一个答案。</p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/Ps5mhRXSTWCupeU.png" alt="image.png" /></p>
<p>简单得说，当需要使用分类算法，且数据比较大的时候就可以尝试使用KNN算法进行分类了。</p>
<p>OK，本次先对KNN算法做一个介绍，下一节解析sklearn的参数，以及K值选取。</p>
<h1 id="knn参数概述"><a class="markdownIt-Anchor" href="#knn参数概述"></a> KNN参数概述</h1>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/248634166">参数详解</a></p>
<p>要使用sklearnKNN算法进行分类，我们需要先了解sklearnKNN算法的一些基本参数，那么这节就先介绍这些内容吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KNeighborsClassifier</span>(<span class="params">n_neighbors = <span class="number">5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       weights=<span class="string">&#x27;uniform&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       algorithm = <span class="string">&#x27;&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       leaf_size = <span class="string">&#x27;30&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       p = <span class="number">2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       metric = <span class="string">&#x27;minkowski&#x27;</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                       metric_params = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                       n_jobs = None</span></span></span><br><span class="line"><span class="function"><span class="params">                       </span>)</span></span><br><span class="line"><span class="function">										</span></span><br><span class="line"><span class="function">- <span class="title">n_neighbors</span>：这个值就是指 <span class="title">KNN</span> 中的 “<span class="title">K</span>”了。前面说到过，通过调整 <span class="title">K</span> 值，算法会有不同的效果。</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">- <span class="title">weights</span>（权重）：最普遍的 <span class="title">KNN</span> 算法无论距离如何，权重都一样，但有时候我们想搞点特殊化，比如距离更近的点让它更加重要。这时候就需要 <span class="title">weight</span> 这个参数了，这个参数有三个可选参数的值，决定了如何分配权重。参数选项如下：</span></span><br><span class="line"><span class="function">        • &#x27;<span class="title">uniform</span>&#x27;：不管远近权重都一样，就是最普通的 <span class="title">KNN</span> 算法的形式。</span></span><br><span class="line"><span class="function">        • &#x27;<span class="title">distance</span>&#x27;：权重和距离成反比，距离预测目标越近具有越高的权重。</span></span><br><span class="line"><span class="function">        • 自定义函数：自定义一个函数，根据输入的坐标值返回对应的权重，达到自定义权重的目的。</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">- <span class="title">algorithm</span>：在 <span class="title">sklearn</span> 中，要构建 <span class="title">KNN</span> 模型有三种构建方式:</span></span><br><span class="line">    	<span class="number">1.</span> 暴力法，就是直接计算距离存储比较的那种放松。</span><br><span class="line">    	<span class="number">2.</span> 使用 kd 树构建 KNN 模型 </span><br><span class="line">    	<span class="number">3.</span> 使用球树构建。 其中暴力法适合数据较小的方式，否则效率会比较低。如果数据量比较大一般会选择用 KD 树构建 KNN 模型，而当 KD 树也比较慢的时候，则可以试试球树来构建 KNN。参数选项如下：</span><br><span class="line">		• <span class="string">&#x27;brute&#x27;</span> ：蛮力实现</span><br><span class="line">		• <span class="string">&#x27;kd_tree&#x27;</span>：KD 树实现 KNN</span><br><span class="line">		• <span class="string">&#x27;ball_tree&#x27;</span>：球树实现 KNN </span><br><span class="line">		• <span class="string">&#x27;auto&#x27;</span>： 默认参数，自动选择合适的方法构建模型</span><br><span class="line">		不过当数据较小或比较稀疏时，无论选择哪个最后都会使用 <span class="string">&#x27;brute&#x27;</span></span><br><span class="line">		</span><br><span class="line">- leaf_size：如果是选择蛮力实现，那么这个值是可以忽略的，当使用KD树或球树，它就是是停止建子树的叶子节点数量的阈值。默认<span class="number">30</span>，但如果数据量增多这个参数需要增大，否则速度过慢不说，还容易过拟合。</span><br><span class="line"></span><br><span class="line">- p：和metric结合使用的，当metric参数是<span class="string">&quot;minkowski&quot;</span>的时候，p=<span class="number">1</span>为曼哈顿距离， p=<span class="number">2</span>为欧式距离。默认为p=<span class="number">2</span>。</span><br><span class="line"></span><br><span class="line">- metric：指定距离度量方法，一般都是使用欧式距离。</span><br><span class="line">		• <span class="string">&#x27;euclidean&#x27;</span> ：欧式距离</span><br><span class="line">		• <span class="string">&#x27;manhattan&#x27;</span>：曼哈顿距离</span><br><span class="line">		• <span class="string">&#x27;chebyshev&#x27;</span>：切比雪夫距离</span><br><span class="line">		• <span class="string">&#x27;minkowski&#x27;</span>： 闵可夫斯基距离，默认参数</span><br><span class="line">        </span><br><span class="line">- n_jobs：指定多少个CPU进行运算，默认是<span class="number">-1</span>，也就是全部都算。</span><br><span class="line"></span><br><span class="line">返回值为：</span><br><span class="line">X_train	划分出的训练集数据（返回值）</span><br><span class="line">X_test	划分出的测试集数据（返回值）</span><br><span class="line">y_train	划分出的训练集标签（返回值）</span><br><span class="line">y_test	划分出的测试集标签（返回值）</span><br><span class="line">knn为分类算法，所以一个数据有其数据值和数据所属的分类即标签</span><br><span class="line">x_train即训练数据集合，y_train即训练数据集合所属的标签</span><br><span class="line">x_test即测试数据集合，y_test即测试数据集合所属的标签</span><br><span class="line"></span><br><span class="line">利用训练数据得出不同的分类标准，即不同的参数，拟合出不同的分类器。</span><br><span class="line">利用测试集数据进行测试，得出测试集数据的预测，然后和测试集体的标签进行比较，得出拟合度最高的分类器。</span><br></pre></td></tr></table></figure>
<h1 id="二-knn代码实例"><a class="markdownIt-Anchor" href="#二-knn代码实例"></a> 二. KNN代码实例</h1>
<p>KNN算法算是机器学习里面最简单的算法之一了，我们来sklearn官方给出的例子，来看看KNN应该怎样使用吧：</p>
<p>数据集使用的是著名的鸢尾花数据集，用KNN来对它做分类。我们先看看鸢尾花长的啥样。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/DV7RsLv1qjnu9KN.png" alt="image.png" /></p>
<p>上面这个就是鸢尾花了，这个鸢尾花数据集主要包含了鸢尾花的花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性（特征），以及鸢尾花卉属于『Setosa，Versicolour，Virginica』三个种类中的哪一类（这三种都长什么样我也不知道）。</p>
<p>在使用KNN算法之前，我们要先决定K的值是多少，要选出最优的K值，可以使用sklearn中的交叉验证方法，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection  <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">k_range = range(<span class="number">1</span>, <span class="number">31</span>)</span><br><span class="line">k_error = []</span><br><span class="line"><span class="comment">#循环，取k=1到k=31，查看误差效果</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_range:</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    <span class="comment">#cv参数决定数据集划分比例，这里是按照5:1划分训练集和测试集</span></span><br><span class="line">    scores = cross_val_score(knn, x, y, cv=<span class="number">6</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    k_error.append(<span class="number">1</span> - scores.mean())</span><br><span class="line"></span><br><span class="line"><span class="comment">#画图，x轴为k值，y值为误差值</span></span><br><span class="line">plt.plot(k_range, k_error)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Value of K for KNN&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>运行后，我们可以得到下面这样的图：<br />
<img src= "/img/loading.gif" data-lazy-src="https://i.loli.net/2020/11/12/2c3tSinaXPLTflv.png" alt="image.png" /><br />
有了这张图，我们就能明显看出K值取多少的时候误差最小，这里明显是K=11最好。当然在实际问题中，如果数据集比较大，那为减少训练时间，K的取值范围可以缩小。</p>
<p>有了K值我们就能运行KNN算法了，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets</span><br><span class="line"></span><br><span class="line">n_neighbors = <span class="number">11</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入一些要玩的数据</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">x = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 我们只采用前两个feature,方便画图在二维平面显示</span></span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># 网格中的步长</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建彩色的图</span></span><br><span class="line">cmap_light = ListedColormap([<span class="string">&#x27;#FFAAAA&#x27;</span>, <span class="string">&#x27;#AAFFAA&#x27;</span>, <span class="string">&#x27;#AAAAFF&#x27;</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">&#x27;#FF0000&#x27;</span>, <span class="string">&#x27;#00FF00&#x27;</span>, <span class="string">&#x27;#0000FF&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#weights是KNN模型中的一个参数，上述参数介绍中有介绍，这里绘制两种权重参数下KNN的效果图</span></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>]:</span><br><span class="line">    <span class="comment"># 创建了一个knn分类器的实例，并拟合数据。</span></span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制决策边界。为此，我们将为每个分配一个颜色</span></span><br><span class="line">    <span class="comment"># 来绘制网格中的点 [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = x[:, <span class="number">0</span>].min() - <span class="number">1</span>, x[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = x[:, <span class="number">1</span>].min() - <span class="number">1</span>, x[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将结果放入一个彩色图中</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制训练点</span></span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, cmap=cmap_bold)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">&quot;3-Class classification (k = %i, weights = &#x27;%s&#x27;)&quot;</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="knn和kmeans"><a class="markdownIt-Anchor" href="#knn和kmeans"></a> KNN和Kmeans</h2>
<p>前面说到过，KNN和Kmeans听起来有些像，但本质是有区别的，这里我们就顺便说一下两者的异同吧。</p>
<p>相同：</p>
<ol>
<li>K值都是重点</li>
<li>都需要计算平面中点的距离</li>
</ol>
<p>相异：<br />
Knn和Kmeans的核心都是通过计算空间中点的距离来实现目的，只是他们的目的是不同的。KNN的最终目的是分类，而Kmeans的目的是给所有距离相近的点分配一个类别，也就是聚类。</p>
<p>简单说，就是画一个圈，KNN是让进来圈子里的人变成自己人，Kmeans是让原本在圈内的人归成一类人。</p>
<p>以上</p>
<h1 id="上课knn代码"><a class="markdownIt-Anchor" href="#上课knn代码"></a> 上课knn代码</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br><span class="line">iris.data</span><br><span class="line"><span class="comment"># 能参与运算的单纯的数据的数据</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iris.data.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iris.target <span class="comment">#分类的个数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iris.target.shape <span class="comment">#150个</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train,X_test,y_train,y_test= train_test_split(iris.data,iris.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"><span class="comment">#第一个参数放的是存粹的能参与运算的数据，即待划分的数据</span></span><br><span class="line"><span class="comment">#第二个参数是已经分类好的带标签的数据</span></span><br><span class="line"><span class="comment">#第三个参数为样本占比，如果是整数的话就是样本数量,即测试集占比25%</span></span><br><span class="line"><span class="comment">#第四个参数为随机数的种子，保证每次都是同一个随机数。若为0或不填，则每次得到数据都不一样</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>train_test_split()函数是用来随机划分样本数据为训练集和测试集的，当然也可以人为的切片划分。函数返回值为划分好的训练集测试集样本和训练集测试集标签。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss = StandardScaler()</span><br><span class="line">X_train = ss.fit_transform(X_train)</span><br><span class="line">X_test = ss.transform(X_test)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意这是数据预处理中的方法：</p>
<ul>
<li><strong>Fit():</strong> Method calculates the parameters μ and σ and saves them as internal objects.</li>
</ul>
<p>解释：简单来说，就是求得训练集X的均值啊，方差啊，最大值啊，最小值啊这些训练集X固有的属性。可以理解为一个训练过程</p>
<ul>
<li><strong>Transform():</strong> Method using these calculated parameters apply the transformation to a particular dataset.</li>
</ul>
<p>解释：在Fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如PCA，StandardScaler等）。</p>
<ul>
<li><strong>Fit_transform():</strong> joins the fit() and transform() method for transformation of dataset.</li>
</ul>
<p>解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。</p>
<p>transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）</p>
<p>fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。</p>
<p>根据对之前部分trainData进行fit的整体指标，对剩余的数据（testData）使用同样的均值、方差、最大最小值等指标进行转换transform(testData)，**从而保证train、test处理方式相同。**所以，一般都是这么用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">sc.fit_tranform(X_train)</span><br><span class="line">sc.tranform(X_test)</span><br></pre></td></tr></table></figure>
<h2 id="note"><a class="markdownIt-Anchor" href="#note"></a> <strong>Note:</strong></h2>
<ul>
<li>必须先用fit_transform(trainData)，之后再transform(testData)</li>
<li>如果直接transform(testData)，程序会报错</li>
<li>如果fit_transfrom(trainData)后，使用fit_transform(testData)而不transform(testData)，虽然也能归一化，但是两个结果不是在同一个“标准”下的，具有明显差异。(<strong>一定要避免这种情况</strong>)</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#knn方法的使用，K值范围为3~10，超过了10严重影响了算法执行的效率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">knc = KNeighborsClassifier()</span><br><span class="line">knc.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_predict = knc.predict(X_test)</span><br><span class="line">print(<span class="string">&#x27;THe accuracy of K-Nearest Neighbor Classifier is&#x27;</span>,knc.score(X_test,y_test))</span><br><span class="line"><span class="comment"># 使用模型自带的评估函数进行准确性测评。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(y_test,y_predict,target_names=iris.target_names))	</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)</strong></p>
<ul>
<li><code>y_true</code>：1 维数组，真实数据的分类标签</li>
<li><code>y_pred</code>：1 维数组，模型预测的分类标签</li>
<li><code>labels</code>：列表，需要评估的标签名称</li>
<li><code>target_names</code>：列表，指定标签名称</li>
<li><code>sample_weight</code>：1 维数组，不同数据点在评估结果中所占的权重</li>
<li><code>digits</code>：评估报告中小数点的保留位数，如果 <code>output_dict=True</code>，此参数不起作用，返回的数值不作处理</li>
<li><code>output_dict</code>：若真，评估结果以字典形式返回</li>
</ul>
<h5 id="返回"><a class="markdownIt-Anchor" href="#返回"></a> 返回</h5>
<p>字符串或字典。</p>
<p>每个分类标签的精确度，召回率和 F1-score。</p>
<ul>
<li><strong>精确度：precision</strong>，正确预测为正的，占全部预测为正的比例，TP / (TP+FP)</li>
<li><strong>召回率：recall</strong>，正确预测为正的，占全部实际为正的比例，TP / (TP+FN)</li>
<li><strong>F1-score</strong>：精确率和召回率的调和平均数，2 * precision*recall / (precision+recall)</li>
</ul>
<p>同时还会给出总体的微平均值，宏平均值和加权平均值。</p>
<ul>
<li>微平均值：micro average，所有数据结果的平均值</li>
<li>宏平均值：macro average，所有标签结果的平均值</li>
<li>加权平均值：weighted average，所有标签结果的加权平均值</li>
</ul>
<p>在二分类场景中，正标签的召回率称为<strong>敏感度（sensitivity）</strong>，负标签的召回率称为<strong>特异性（specificity）</strong>。</p>
</blockquote>
<h1 id="上课knn代码-2"><a class="markdownIt-Anchor" href="#上课knn代码-2"></a> 上课knn代码</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_data</span>():</span></span><br><span class="line">    features = np.array( [[<span class="number">2.88</span>, <span class="number">3.05</span>], [<span class="number">3.1</span>, <span class="number">2.45</span>], [<span class="number">3.05</span>, <span class="number">2.8</span>], [<span class="number">2.9</span>, <span class="number">2.7</span>], [<span class="number">2.75</span>, <span class="number">3.4</span>],</span><br><span class="line">         [<span class="number">3.23</span>, <span class="number">2.9</span>], [<span class="number">3.2</span>, <span class="number">3.75</span>], [<span class="number">3.5</span>, <span class="number">2.9</span>], [<span class="number">3.65</span>, <span class="number">3.6</span>], [<span class="number">3.35</span>, <span class="number">3.3</span>]])</span><br><span class="line">    labels = [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">features, labels = create_data()</span><br><span class="line">print(<span class="string">&#x27;features: \n&#x27;</span>, features)</span><br><span class="line">print(<span class="string">&#x27;labels:\n&#x27;</span>, labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line"></span><br><span class="line">x_features = list(map(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))</span><br><span class="line">y_features = list(map(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.scatter(x_features[:<span class="number">5</span>], y_features[:<span class="number">5</span>], c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">plt.scatter(x_features[<span class="number">5</span>:], y_features[<span class="number">5</span>:], c=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_man</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.sum(np.abs(x - y))</span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">3.1</span>,<span class="number">3.2</span>])</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">2.5</span>, <span class="number">2.8</span>])</span><br><span class="line"></span><br><span class="line">d_man = d_man(x, y)</span><br><span class="line">print(d_man)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_euc</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    d = np.sqrt(np.sum(np.square(x-y)))</span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.random(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>, x)</span><br><span class="line"></span><br><span class="line">y = np.random.random(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>, y)</span><br><span class="line"></span><br><span class="line">distance_euc = d_euc(x, y)</span><br><span class="line">print(distance_euc)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">marority_voting</span>(<span class="params">class_count</span>):</span></span><br><span class="line">    sorted_class_count = sorted(</span><br><span class="line">        class_count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> sorted_class_count</span><br><span class="line"></span><br><span class="line">arr = &#123;<span class="string">&#x27;A&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>:<span class="number">2</span>, <span class="string">&quot;c&quot;</span>:<span class="number">6</span>, <span class="string">&quot;D&quot;</span>:<span class="number">5</span>&#125;</span><br><span class="line">marority_voting(arr)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现自己的knn算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn_classify</span>(<span class="params">test_data, train_data, labels, k</span>):</span></span><br><span class="line">    distances = np.array([])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> each_data <span class="keyword">in</span> train_data:</span><br><span class="line">        d = d_euc(test_data, each_data)</span><br><span class="line">        distances = np.append(distances, d)</span><br><span class="line">    </span><br><span class="line">    sorted_distance_index = distances.argsort()</span><br><span class="line">    sorted_distance = np.sort(distances)</span><br><span class="line">    r = (sorted_distance[k]+sorted_distance[k<span class="number">-1</span>])/<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    class_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        vote_label = labels[sorted_distance_index[i]]</span><br><span class="line">        class_count[vote_label] = class_count.get(vote_label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    final_label = marority_voting(class_count)</span><br><span class="line">    <span class="keyword">return</span> final_label, r</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_data = np.array([<span class="number">3.18</span>, <span class="number">3.15</span>])</span><br><span class="line">final_label, r = knn_classify(test_data, features, labels, <span class="number">5</span>)</span><br><span class="line">final_label</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">circle</span>(<span class="params">r, a, b</span>):</span></span><br><span class="line">    theta = np.arange(<span class="number">0</span>, <span class="number">2</span>*np.pi, <span class="number">0.01</span>)</span><br><span class="line">    x = a+r *np.cos(theta)</span><br><span class="line">    y = b+r*np.sin(theta)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">k_circle_x, k_circle_y = circle(r, <span class="number">3.18</span>, <span class="number">3.15</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.xlim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line">plt.ylim((<span class="number">2.4</span>, <span class="number">3.8</span>))</span><br><span class="line"></span><br><span class="line">x_features = list(map(<span class="keyword">lambda</span> x: x[<span class="number">0</span>], features))</span><br><span class="line">y_features = list(map(<span class="keyword">lambda</span> y: y[<span class="number">1</span>], features))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.scatter(x_features[:<span class="number">5</span>], y_features[:<span class="number">5</span>], c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">plt.scatter(x_features[<span class="number">5</span>:], y_features[<span class="number">5</span>:], c=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">plt.scatter([<span class="number">3.18</span>], [<span class="number">3.15</span>], c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(k_circle_x, k_circle_y)</span><br></pre></td></tr></table></figure>
<h1 id="朴素贝叶斯"><a class="markdownIt-Anchor" href="#朴素贝叶斯"></a> 朴素贝叶斯</h1>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database">作业</a></p>
<h1 id="决策树"><a class="markdownIt-Anchor" href="#决策树"></a> 决策树</h1>
<h1 id="kmeans"><a class="markdownIt-Anchor" href="#kmeans"></a> kmeans</h1>
<h1 id="randomforest"><a class="markdownIt-Anchor" href="#randomforest"></a> randomForest</h1>
<h1 id="深度学习常用数学概念"><a class="markdownIt-Anchor" href="#深度学习常用数学概念"></a> 深度学习常用数学概念</h1>
<ul>
<li>
<p>高斯函数</p>
<ul>
<li>高斯函数的不定积分是误差函数</li>
<li>核函数</li>
</ul>
</li>
<li>
<p>径向基函数</p>
</li>
<li>
<p>高斯径向基函数</p>
</li>
<li>
<p>范数</p>
</li>
<li>
<p>方向导数</p>
<p>算法围绕哪个方向进行</p>
</li>
</ul>
</div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/ai%E7%AE%97%E6%B3%95/">ai算法</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/08/05/GRYrANb2noBKXMh.jpg" data-sites="wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/12/PythonStudy/"><img class="prev-cover" data-lazy-src="https://i.loli.net/2020/08/07/nVC67rzJvYjW8xc.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PythonStudy</div></div></a></div><div class="next-post pull-right"><a href="/2020/10/14/%E8%AE%A1%E7%BD%91%E5%AE%9E%E9%AA%8C/"><img class="next-cover" data-lazy-src="https://i.loli.net/2020/08/07/Brt6pdNia5AOF9T.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计网实验</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/11/12/PythonStudy/" title="PythonStudy"><img class="relatedPosts_cover" data-lazy-src="https://i.loli.net/2020/08/07/nVC67rzJvYjW8xc.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-12</div><div class="relatedPosts_title">PythonStudy</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://i.loli.net/2020/08/05/GRYrANb2noBKXMh.jpg)" data-type="photo"><div id="footer-wrap"><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">hi~ welcome to my blog</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script></div></body></html>