---
title: 人工智能导论
date: 2020-11-08 15:50:45
tags:
	- python
	- ai算法
categories: python
typora-root-url: G:\typoraimages
---

# 机器学习中训练集、验证集和测试集的作用

* 训练集

  作用是用来拟合模型，通过设置分类器的参数，训练分类模型。后续结合验证集作用时，会选出同一参数的不同取值，拟合出多个分类器。

* 验证集

  作用是当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。如svn中的参数c和核函数等。

* 测试集

  通过训练集和验证集得出最优模型后，使用测试集进行模型预测。用来衡量该最优模型的性能和分类能力。即可以把测试集当做从来不存在的数据集，当已经确定模型参数后，使用测试集进行模型性能评价。

* 对原始数据进行三个数据集的划分，也是为了防止模型过拟合。当使用了所有的原始数据去训练模型，得到的结果很可能是该模型最大程度地拟合了原始数据，亦即该模型是为了拟合所有原始数据而存在。当新的样本出现，再使用该模型进行预测，效果可能还不如只使用一部分数据训练的模型。

# KNN算法

[搬运](https://www.cnblogs.com/listenfwind/p/10311496.html)

* 算法概述

  KNN可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一，注意KNN算法是有监督学习中的分类算法，它看起来和另一个机器学习算法Kmeans有点像（Kmeans是无监督学习算法），但却是有本质区别的。那么什么是KNN算法呢，接下来我们就来介绍介绍吧。

* knn算法介绍

  KNN的全称是K Nearest Neighbors，意思是K个最近的邻居，从这个名字我们就能看出一些KNN算法的蛛丝马迹了。K个最近邻居，毫无疑问，K的取值肯定是至关重要的。那么最近的邻居又是怎么回事呢？**其实啊，KNN的原理就是当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别**。听起来有点绕，还是看看图吧。

  ![KNN原理1](https://img2018.cnblogs.com/blog/1011838/201901/1011838-20190125174428395-6404591.png)

  图中绿色的点就是我们要预测的那个点，假设K=3。那么KNN算法就会找到与它距离最近的三个点（这里用圆圈把它圈起来了），看看哪种类别多一些，比如这个例子中是蓝色三角形多一些，新来的绿色点就归类到蓝三角了。

  ![KNN原理2](https://img2018.cnblogs.com/blog/1011838/201901/1011838-20190125174434360-1539590461.png)

  但是，**当K=5的时候，判定就变成不一样了**。这次变成红圆多一些，所以新来的绿点被归类成红圆。从这个例子中，我们就能看得出K的取值是很重要的。

  明白了大概原理后，我们就来说一说细节的东西吧，主要有两个，**K值的选取**和**点距离的计算**。

* 距离计算

  要度量空间中点距离的话，有好几种度量方式，比如常见的曼哈顿距离计算，欧式距离计算等等。不过通常KNN算法中使用的是欧式距离，这里只是简单说一下，拿二维平面为例，，二维空间两个点的欧式距离计算公式如下：

  ![二维空间欧式距离](https://img2018.cnblogs.com/blog/1011838/201811/1011838-20181105210120839-1494903025.jpg)

  这个高中应该就有接触到的了，其实就是计算（x1,y1）和（x2,y2）的距离。拓展到多维空间，则公式变成这样：

  ![多维空间欧式距离](https://img2018.cnblogs.com/blog/1011838/201811/1011838-20181105210113366-1125611006.jpg)

  这样我们就明白了如何计算距离，KNN算法最简单粗暴的就是将预测点与所有点距离进行计算，然后保存并排序，选出前面K个值看看哪些类别比较多。但其实也可以通过一些数据结构来辅助，比如最大堆，这里就不多做介绍，有兴趣可以百度最大堆相关数据结构的知识。

* k值选择

  通过上面那张图我们知道K的取值比较重要，那么该如何确定K取多少值好呢？答案是通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。

  通过交叉验证计算方差后你大致会得到下面这样的图：
  ![K值与Error](https://img2018.cnblogs.com/blog/1011838/201904/1011838-20190401220304846-2066630053.png)

  这个图其实很好理解，当你增大k的时候，一般错误率会先降低，因为有周围更多的样本可以借鉴了，分类效果会变好。但注意，和K-means不一样，当K值更大的时候，错误率会更高。这也很好理解，比如说你一共就35个样本，当你K增大到30的时候，KNN基本上就没意义了。

  所以选择K点的时候可以选择一个较大的临界K点，当它继续增大或减小的时候，错误率都会上升，比如图中的K=10。具体如何得出K最佳值的代码，下一节的代码实例中会介绍。

* knn特点

  KNN是一种**非参的**，**惰性**的算法模型。什么是非参，什么是惰性呢？

  **非参**的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。

  **惰性**又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。

  ## KNN算法的优势和劣势

  了解KNN算法的优势和劣势，可以帮助我们在选择学习算法的时候做出更加明智的决定。那我们就来看看KNN算法都有哪些优势以及其缺陷所在！

  #### KNN算法优点

  1. 简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。
  2. 模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。
  3. 预测效果好。
  4. 对异常值不敏感

  #### KNN算法缺点

  1. 对内存要求较高，因为该算法存储了所有训练数据
  2. 预测阶段可能很慢
  3. 对不相关的功能和数据规模敏感

  至于什么时候应该选择使用KNN算法，sklearn的这张图给了我们一个答案。

![sklearn算法选择](https://img2018.cnblogs.com/blog/1011838/201901/1011838-20190123203347054-1083715070.png)

简单得说，当需要使用分类算法，且数据比较大的时候就可以尝试使用KNN算法进行分类了。

OK，本次先对KNN算法做一个介绍，下一节解析sklearn的参数，以及K值选取。

# KNN参数概述

[参数详解](https://zhuanlan.zhihu.com/p/248634166)

要使用sklearnKNN算法进行分类，我们需要先了解sklearnKNN算法的一些基本参数，那么这节就先介绍这些内容吧

```python
def KNeighborsClassifier(n_neighbors = 5,
                       weights='uniform',
                       algorithm = '',
                       leaf_size = '30',
                       p = 2,
                       metric = 'minkowski',
                       metric_params = None,
                       n_jobs = None
                       )
										
- n_neighbors：这个值就是指 KNN 中的 “K”了。前面说到过，通过调整 K 值，算法会有不同的效果。

- weights（权重）：最普遍的 KNN 算法无论距离如何，权重都一样，但有时候我们想搞点特殊化，比如距离更近的点让它更加重要。这时候就需要 weight 这个参数了，这个参数有三个可选参数的值，决定了如何分配权重。参数选项如下：
        • 'uniform'：不管远近权重都一样，就是最普通的 KNN 算法的形式。
        • 'distance'：权重和距离成反比，距离预测目标越近具有越高的权重。
        • 自定义函数：自定义一个函数，根据输入的坐标值返回对应的权重，达到自定义权重的目的。

- algorithm：在 sklearn 中，要构建 KNN 模型有三种构建方式:
    	1. 暴力法，就是直接计算距离存储比较的那种放松。
    	2. 使用 kd 树构建 KNN 模型 
    	3. 使用球树构建。 其中暴力法适合数据较小的方式，否则效率会比较低。如果数据量比较大一般会选择用 KD 树构建 KNN 模型，而当 KD 树也比较慢的时候，则可以试试球树来构建 KNN。参数选项如下：
		• 'brute' ：蛮力实现
		• 'kd_tree'：KD 树实现 KNN
		• 'ball_tree'：球树实现 KNN 
		• 'auto'： 默认参数，自动选择合适的方法构建模型
		不过当数据较小或比较稀疏时，无论选择哪个最后都会使用 'brute'
		
- leaf_size：如果是选择蛮力实现，那么这个值是可以忽略的，当使用KD树或球树，它就是是停止建子树的叶子节点数量的阈值。默认30，但如果数据量增多这个参数需要增大，否则速度过慢不说，还容易过拟合。

- p：和metric结合使用的，当metric参数是"minkowski"的时候，p=1为曼哈顿距离， p=2为欧式距离。默认为p=2。

- metric：指定距离度量方法，一般都是使用欧式距离。
		• 'euclidean' ：欧式距离
		• 'manhattan'：曼哈顿距离
		• 'chebyshev'：切比雪夫距离
		• 'minkowski'： 闵可夫斯基距离，默认参数
        
- n_jobs：指定多少个CPU进行运算，默认是-1，也就是全部都算。

返回值为：
X_train	划分出的训练集数据（返回值）
X_test	划分出的测试集数据（返回值）
y_train	划分出的训练集标签（返回值）
y_test	划分出的测试集标签（返回值）
knn为分类算法，所以一个数据有其数据值和数据所属的分类即标签
x_train即训练数据集合，y_train即训练数据集合所属的标签
x_test即测试数据集合，y_test即测试数据集合所属的标签

利用训练数据得出不同的分类标准，即不同的参数，拟合出不同的分类器。
利用测试集数据进行测试，得出测试集数据的预测，然后和测试集体的标签进行比较，得出拟合度最高的分类器。
```

# 二. KNN代码实例

KNN算法算是机器学习里面最简单的算法之一了，我们来sklearn官方给出的例子，来看看KNN应该怎样使用吧：

数据集使用的是著名的鸢尾花数据集，用KNN来对它做分类。我们先看看鸢尾花长的啥样。

![鸢尾花](https://img2018.cnblogs.com/blog/1011838/201902/1011838-20190216144242837-1180208363.png)

上面这个就是鸢尾花了，这个鸢尾花数据集主要包含了鸢尾花的花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性（特征），以及鸢尾花卉属于『Setosa，Versicolour，Virginica』三个种类中的哪一类（这三种都长什么样我也不知道）。

在使用KNN算法之前，我们要先决定K的值是多少，要选出最优的K值，可以使用sklearn中的交叉验证方法，代码如下：

```python
from sklearn.datasets import load_iris
from sklearn.model_selection  import cross_val_score
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

#读取鸢尾花数据集
iris = load_iris()
x = iris.data
y = iris.target
k_range = range(1, 31)
k_error = []
#循环，取k=1到k=31，查看误差效果
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    #cv参数决定数据集划分比例，这里是按照5:1划分训练集和测试集
    scores = cross_val_score(knn, x, y, cv=6, scoring='accuracy')
    k_error.append(1 - scores.mean())

#画图，x轴为k值，y值为误差值
plt.plot(k_range, k_error)
plt.xlabel('Value of K for KNN')
plt.ylabel('Error')
plt.show()
```

运行后，我们可以得到下面这样的图：
![KNN Error](https://img2018.cnblogs.com/blog/1011838/201904/1011838-20190403095416261-295995970.png)
有了这张图，我们就能明显看出K值取多少的时候误差最小，这里明显是K=11最好。当然在实际问题中，如果数据集比较大，那为减少训练时间，K的取值范围可以缩小。

有了K值我们就能运行KNN算法了，具体代码如下：

```python
import matplotlib.pyplot as plt
from numpy import *
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 11

# 导入一些要玩的数据
iris = datasets.load_iris()
x = iris.data[:, :2]  # 我们只采用前两个feature,方便画图在二维平面显示
y = iris.target


h = .02  # 网格中的步长

# 创建彩色的图
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])


#weights是KNN模型中的一个参数，上述参数介绍中有介绍，这里绘制两种权重参数下KNN的效果图
for weights in ['uniform', 'distance']:
    # 创建了一个knn分类器的实例，并拟合数据。
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(x, y)

    # 绘制决策边界。为此，我们将为每个分配一个颜色
    # 来绘制网格中的点 [x_min, x_max]x[y_min, y_max].
    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # 将结果放入一个彩色图中
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # 绘制训练点
    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n_neighbors, weights))

plt.show()
```

## KNN和Kmeans

前面说到过，KNN和Kmeans听起来有些像，但本质是有区别的，这里我们就顺便说一下两者的异同吧。

相同：

1. K值都是重点
2. 都需要计算平面中点的距离

相异：
Knn和Kmeans的核心都是通过计算空间中点的距离来实现目的，只是他们的目的是不同的。KNN的最终目的是分类，而Kmeans的目的是给所有距离相近的点分配一个类别，也就是聚类。

简单说，就是画一个圈，KNN是让进来圈子里的人变成自己人，Kmeans是让原本在圈内的人归成一类人。

以上

# 上课knn代码

```python
from sklearn.datasets import load_iris
iris = load_iris()
iris.data
# 能参与运算的单纯的数据的数据
```

```python
iris.data.shape
```

```python
iris.target #分类的个数
```

```python
iris.target.shape #150个
```

```python
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test= train_test_split(iris.data,iris.target,test_size=0.25,random_state=33)
#第一个参数放的是存粹的能参与运算的数据，即待划分的数据
#第二个参数是已经分类好的带标签的数据
#第三个参数为样本占比，如果是整数的话就是样本数量,即测试集占比25%
#第四个参数为随机数的种子，保证每次都是同一个随机数。若为0或不填，则每次得到数据都不一样
```

> train_test_split()函数是用来随机划分样本数据为训练集和测试集的，当然也可以人为的切片划分。函数返回值为划分好的训练集测试集样本和训练集测试集标签。

```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)
```

> 注意这是数据预处理中的方法：
>
> - **Fit():** Method calculates the parameters μ and σ and saves them as internal objects.
>
> 解释：简单来说，就是求得训练集X的均值啊，方差啊，最大值啊，最小值啊这些训练集X固有的属性。可以理解为一个训练过程
>
> - **Transform():** Method using these calculated parameters apply the transformation to a particular dataset.
>
> 解释：在Fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如PCA，StandardScaler等）。
>
> - **Fit_transform():** joins the fit() and transform() method for transformation of dataset.
>
> 解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。
>
> transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）
>
> fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。
>
> 根据对之前部分trainData进行fit的整体指标，对剩余的数据（testData）使用同样的均值、方差、最大最小值等指标进行转换transform(testData)，**从而保证train、test处理方式相同。**所以，一般都是这么用：
>
> ```python
> from sklearn.preprocessing import StandardScaler
> sc = StandardScaler()
> sc.fit_tranform(X_train)
> sc.tranform(X_test)
> ```
>
> ## **Note:**
>
> - 必须先用fit_transform(trainData)，之后再transform(testData)
> - 如果直接transform(testData)，程序会报错
> - 如果fit_transfrom(trainData)后，使用fit_transform(testData)而不transform(testData)，虽然也能归一化，但是两个结果不是在同一个“标准”下的，具有明显差异。(**一定要避免这种情况**)

```python
#knn方法的使用，K值范围为3~10，超过了10严重影响了算法执行的效率
from sklearn.neighbors import KNeighborsClassifier

knc = KNeighborsClassifier()
knc.fit(X_train,y_train)
```

```python
y_predict = knc.predict(X_test)
print('THe accuracy of K-Nearest Neighbor Classifier is',knc.score(X_test,y_test))
# 使用模型自带的评估函数进行准确性测评。
```

```python
from sklearn.metrics import classification_report
print(classification_report(y_test,y_predict,target_names=iris.target_names))	
```

> **sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)**
>
> - `y_true`：1 维数组，真实数据的分类标签
> - `y_pred`：1 维数组，模型预测的分类标签
> - `labels`：列表，需要评估的标签名称
> - `target_names`：列表，指定标签名称
> - `sample_weight`：1 维数组，不同数据点在评估结果中所占的权重
> - `digits`：评估报告中小数点的保留位数，如果 `output_dict=True`，此参数不起作用，返回的数值不作处理
> - `output_dict`：若真，评估结果以字典形式返回
>
> ##### 返回
>
> 字符串或字典。
>
> 每个分类标签的精确度，召回率和 F1-score。
>
> - **精确度：precision**，正确预测为正的，占全部预测为正的比例，TP / (TP+FP)
> - **召回率：recall**，正确预测为正的，占全部实际为正的比例，TP / (TP+FN)
> - **F1-score**：精确率和召回率的调和平均数，2 * precision*recall / (precision+recall)
>
> 同时还会给出总体的微平均值，宏平均值和加权平均值。
>
> - 微平均值：micro average，所有数据结果的平均值
> - 宏平均值：macro average，所有标签结果的平均值
> - 加权平均值：weighted average，所有标签结果的加权平均值
>
> 在二分类场景中，正标签的召回率称为**敏感度（sensitivity）**，负标签的召回率称为**特异性（specificity）**。

# 上课knn代码

```python
import numpy as np

def create_data():
    features = np.array( [[2.88, 3.05], [3.1, 2.45], [3.05, 2.8], [2.9, 2.7], [2.75, 3.4],
         [3.23, 2.9], [3.2, 3.75], [3.5, 2.9], [3.65, 3.6], [3.35, 3.3]])
    labels = ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']
    return features, labels
```

```python
features, labels = create_data()
print('features: \n', features)
print('labels:\n', labels)
```

```python
from matplotlib import pyplot as plt
%matplotlib inline

plt.figure(figsize=(5,5))
plt.xlim((2.4, 3.8))
plt.ylim((2.4, 3.8))

x_features = list(map(lambda x: x[0], features))
y_features = list(map(lambda y: y[1], features))


plt.scatter(x_features[:5], y_features[:5], c='b')
plt.scatter(x_features[5:], y_features[5:], c='g')
plt.scatter([3.18], [3.15], c='r', marker='x')
```

```python
def d_man(x, y):
    d = np.sum(np.abs(x - y))
    return d
```

```python
x = np.array([3.1,3.2])

y = np.array([2.5, 2.8])

d_man = d_man(x, y)
print(d_man)
```

```python
def d_euc(x, y):
    d = np.sqrt(np.sum(np.square(x-y)))
    return d
```

```python
x = np.random.random(10)
print("x:", x)

y = np.random.random(10)
print("y:", y)

distance_euc = d_euc(x, y)
print(distance_euc)
```

```python
import operator


def marority_voting(class_count):
    sorted_class_count = sorted(
        class_count.items(), key=operator.itemgetter(1), reverse=True
    )
    return sorted_class_count

arr = {'A':3, 'B':2, "c":6, "D":5}
marority_voting(arr)
```

```python
# 实现自己的knn算法
def knn_classify(test_data, train_data, labels, k):
    distances = np.array([])
    
    for each_data in train_data:
        d = d_euc(test_data, each_data)
        distances = np.append(distances, d)
    
    sorted_distance_index = distances.argsort()
    sorted_distance = np.sort(distances)
    r = (sorted_distance[k]+sorted_distance[k-1])/2
    
    class_count = {}
    for i in range(k):
        vote_label = labels[sorted_distance_index[i]]
        class_count[vote_label] = class_count.get(vote_label, 0) + 1
    
    final_label = marority_voting(class_count)
    return final_label, r
```

```python
test_data = np.array([3.18, 3.15])
final_label, r = knn_classify(test_data, features, labels, 5)
final_label
```

```python
def circle(r, a, b):
    theta = np.arange(0, 2*np.pi, 0.01)
    x = a+r *np.cos(theta)
    y = b+r*np.sin(theta)
    return x, y
```

```python
k_circle_x, k_circle_y = circle(r, 3.18, 3.15)

plt.figure(figsize=(5,5))
plt.xlim((2.4, 3.8))
plt.ylim((2.4, 3.8))

x_features = list(map(lambda x: x[0], features))
y_features = list(map(lambda y: y[1], features))


plt.scatter(x_features[:5], y_features[:5], c='b')
plt.scatter(x_features[5:], y_features[5:], c='g')
plt.scatter([3.18], [3.15], c='r', marker='x')

plt.plot(k_circle_x, k_circle_y)
```

# 朴素贝叶斯

[作业](https://www.kaggle.com/uciml/pima-indians-diabetes-database)

# 决策树

![image-20201111175628526](/G:/typoraimages/image-20201111175628526.png)